{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1uMZ1vt9oH8_SXyD0INVK5GA6NvOLiwXo","authorship_tag":"ABX9TyMrqgIQsy6nGxCscPvoZxlV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# ðŸ“Œ STEP 1: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M6nZRmWZBqGs","executionInfo":{"status":"ok","timestamp":1745166513460,"user_tz":-330,"elapsed":2804,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}},"outputId":"68292bbf-f3cf-48dd-c37b-a9397d1c0863"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# ðŸ“Œ STEP 2: Install Dependencies\n","!pip install transformers scikit-learn --quiet\n"],"metadata":{"id":"kOJXsDEnB6gc","executionInfo":{"status":"ok","timestamp":1745166521579,"user_tz":-330,"elapsed":3039,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["# ðŸ“Œ STEP 3: Import Required Libraries\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","from sklearn.metrics import classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n"],"metadata":{"id":"KLzcqP2OB8dV","executionInfo":{"status":"ok","timestamp":1745166524083,"user_tz":-330,"elapsed":31,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}}},"execution_count":26,"outputs":[]},{"source":["# ðŸ“Œ STEP 4: Load Your Labeled Dataset from Google Drive\n","data_path = '/content/drive/MyDrive/Code/Restaurent_data/augmented_data_restaurant_bert.csv'  # <- change this\n","df = pd.read_csv(data_path)\n","\n","# Assume your CSV has columns: 'review', aspect, polarity\n","# If 'sentence' is named differently, change it here\n","df['text'] = df['aspect_category'] + \" [ASP] \" + df['aspect_term']"],"cell_type":"code","metadata":{"id":"EsDQ7cXFCct1","executionInfo":{"status":"ok","timestamp":1745166673529,"user_tz":-330,"elapsed":13,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["# ðŸ“Œ STEP 5: Tokenization & Dataset Preparation\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class ReviewDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=128):\n","        self.texts = texts\n","        # Convert string labels to numerical labels\n","        self.labels = [0 if label == \"Negative\" else 1 if label == \"Neutral\" else 2 for label in labels]\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","    def __getitem__(self, idx):\n","        # Ensure the text is a string before tokenization\n","        text = str(self.texts[idx])  # Convert to string if not already\n","        encoded = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_len, return_tensors='pt')\n","        return {\n","            'input_ids': encoded['input_ids'].squeeze(),\n","            'attention_mask': encoded['attention_mask'].squeeze(),\n","            'label': torch.tensor(self.labels[idx], dtype=torch.long)\n","        }\n","\n","# Split the data\n","train_texts, val_texts, train_labels, val_labels = train_test_split(df['text'], df['polarity'], test_size=0.2, random_state=42)\n","\n","# Ensure all elements in train_texts and val_texts are strings\n","train_texts = train_texts.astype(str).tolist()  # Convert to strings and then to list\n","val_texts = val_texts.astype(str).tolist()  # Convert to strings and then to list\n","\n","\n","train_dataset = ReviewDataset(train_texts, train_labels.tolist(), tokenizer)\n","val_dataset = ReviewDataset(val_texts, val_labels.tolist(), tokenizer)\n","\n","train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=32)"],"metadata":{"id":"qCY9tfFcCkFF","executionInfo":{"status":"ok","timestamp":1745166908977,"user_tz":-330,"elapsed":260,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}}},"execution_count":43,"outputs":[]},{"cell_type":"code","source":["# ðŸ“Œ STEP 6: Define BERT Classifier\n","class BERTClassifier(nn.Module):\n","    def __init__(self, hidden_size=768, num_labels=3):\n","        super().__init__()\n","        self.bert = BertModel.from_pretrained('bert-base-uncased')\n","        self.dropout = nn.Dropout(0.3)\n","        self.classifier = nn.Linear(hidden_size, num_labels)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs.pooler_output\n","        return self.classifier(self.dropout(pooled_output))\n"],"metadata":{"id":"4I-9FrhNCoC-","executionInfo":{"status":"ok","timestamp":1745166910396,"user_tz":-330,"elapsed":15,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}}},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["# ðŸ“Œ STEP 7: Model Training\n","# This step was missing and is where train_losses is populated\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model = BERTClassifier().to(device)\n","optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","criterion = nn.CrossEntropyLoss()\n","\n","num_epochs = 5  # Adjust as needed\n","train_losses = [] # Initialize train_losses\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for batch in train_loader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['label'].to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","\n","    avg_loss = total_loss / len(train_loader)\n","    train_losses.append(avg_loss)\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n","\n"],"metadata":{"id":"Ns6DOJFBCp8H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ðŸ“Œ STEP 8: Evaluate Model\n","def evaluate_model():\n","    model.eval()\n","    preds, true = [], []\n","    with torch.no_grad():\n","        for batch in val_loader:\n","            input_ids = batch['input_ids'].to(device)\n","            attention_mask = batch['attention_mask'].to(device)\n","            labels = batch['label'].to(device)\n","\n","            outputs = model(input_ids, attention_mask)\n","            predictions = torch.argmax(outputs, dim=1)\n","            preds.extend(predictions.cpu().numpy())\n","            true.extend(labels.cpu().numpy())\n","\n","    print(\"\\nClassification Report:\")\n","    print(classification_report(true, preds, target_names=[\"Negative\", \"Neutral\", \"Positive\"]))\n","    print(\"Confusion Matrix:\")\n","    print(confusion_matrix(true, preds))\n"],"metadata":{"id":"cZgB7qKRCsLO","executionInfo":{"status":"ok","timestamp":1745166720573,"user_tz":-330,"elapsed":15,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["# ðŸ“Œ STEP 9: Plot Loss\n","plt.plot(train_losses)\n","plt.title(\"Training Loss per Epoch\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.grid(True)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"Tw3pHYMPCtyP","executionInfo":{"status":"error","timestamp":1745166726538,"user_tz":-330,"elapsed":155,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}},"outputId":"05364a97-c541-4d5b-b488-5db5cc649a15"},"execution_count":36,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_losses' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-7dcddf52c8c4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ðŸ“Œ STEP 9: Plot Loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training Loss per Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"]}]},{"cell_type":"code","source":["# ðŸ“Œ STEP 10: Train and Evaluate\n","train_model(epochs=3)\n","evaluate_model()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":176},"id":"zMu_dohYCvNQ","executionInfo":{"status":"error","timestamp":1745166740123,"user_tz":-330,"elapsed":5,"user":{"displayName":"Akash Panda","userId":"14065063236529144716"}},"outputId":"52e18427-f18f-439e-9c49-0716c99655cd"},"execution_count":37,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_model' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-3e8effe64841>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# ðŸ“Œ STEP 10: Train and Evaluate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_model' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"obUDIq9-Cyh-"},"execution_count":null,"outputs":[]}]}